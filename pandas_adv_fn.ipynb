{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f23171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "### TABLE PROCESSING\n",
    "#=============================================================================================\n",
    "\n",
    "def pd_config():\n",
    "    options = {\n",
    "        'display': {\n",
    "            'max_colwidth': 25,\n",
    "            'expand_frame_repr': False,  # Don't wrap to multiple pages\n",
    "            'max_rows': 14,\n",
    "            'max_seq_items': 50,         # Max length of printed sequence\n",
    "            'precision': 4,\n",
    "            'show_dimensions': False\n",
    "        },\n",
    "        'mode': {\n",
    "            'chained_assignment': None   # Controls SettingWithCopyWarning\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for category, option in options.items():\n",
    "        for op, value in option.items():\n",
    "            pd.set_option(f'{category}.{op}', value)  # Python 3.6+\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pd_config()\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        gc.collect()\n",
    "        if (col_type != object) and (str(col_type).lower() != 'category') and ('time' not in str(col_type).lower()):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                for int_type in (np.int8, np.int16, np.int32, np.int64):\n",
    "                    if c_min > np.iinfo(int_type).min and c_max < np.iinfo(int_type).max:\n",
    "                        df[col] = df[col].astype(int_type)\n",
    "            else:\n",
    "                for float_type in (np.float16, np.float32):\n",
    "                    if c_min > np.finfo(float_type).min and c_max < np.finfo(float_type).max:\n",
    "                        df[col] = df[col].astype(float_type)\n",
    "                        break\n",
    "                else:  # break is required with for/else\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            if col_type == object:\n",
    "                df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def list_shuff(items, df):\n",
    "    \"Bring a list of columns to the front\"\n",
    "    cols = list(df)\n",
    "    for i in range(len(items)):\n",
    "        cols.insert(i, cols.pop(cols.index(items[i])))\n",
    "    df = df.loc[:, cols]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "### TABLE EXPLORATION\n",
    "#=============================================================================================\n",
    "\n",
    "\n",
    "def corr_list(df):\n",
    "\n",
    "  return  (df.corr()\n",
    "          .unstack()\n",
    "          .sort_values(kind=\"quicksort\",ascending=False)\n",
    "          .drop_duplicates().iloc[1:]); df_out\n",
    "          \n",
    "\n",
    "def missing_data(data):\n",
    "    \"Create a dataframe with a percentage and count of missing values\"\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "\n",
    "\n",
    "\n",
    "### FEATURE PROCESSING\n",
    "#=============================================================================================\n",
    "\n",
    "def drop_corr(df, thresh=0.99,keep_cols=[]):\n",
    "    df_corr = df.corr().abs()\n",
    "    upper = df_corr.where(np.triu(np.ones(df_corr.shape), k=1).astype(np.bool))\n",
    "    to_remove = [column for column in upper.columns if any(upper[column] > thresh)] ## Change to 99% for selection\n",
    "    to_remove = [x for x in to_remove if x not in keep_cols]\n",
    "    df_corr = df_corr.drop(columns = to_remove)\n",
    "    return df.drop(to_remove,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def replace_small_cat(df, columns, thresh=0.2, term=\"other\"):\n",
    "  for col in columns:\n",
    "\n",
    "    # Step 1: count the frequencies\n",
    "    frequencies = df[col].value_counts(normalize = True)\n",
    "\n",
    "  # Step 2: establish your threshold and filter the smaller categories\n",
    "\n",
    "    small_categories = frequencies[frequencies < thresh].index\n",
    "\n",
    "    df[col] = df[col].replace(small_categories, \"Other\")\n",
    "    \n",
    "  return df\n",
    "\n",
    "\n",
    "\n",
    "def constant_feature_detect(data,threshold=0.98):\n",
    "    \"\"\" detect features that show the same value for the \n",
    "    majority/all of the observations (constant/quasi-constant features)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Dataframe\n",
    "    threshold : threshold to identify the variable as constant\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of variables names\n",
    "    \"\"\"\n",
    "    \n",
    "    data_copy = data.copy(deep=True)\n",
    "    quasi_constant_feature = []\n",
    "    for feature in data_copy.columns:\n",
    "        predominant = (data_copy[feature].value_counts() / np.float(\n",
    "                      len(data_copy))).sort_values(ascending=False).values[0]\n",
    "        if predominant >= threshold:\n",
    "            quasi_constant_feature.append(feature)\n",
    "    print(len(quasi_constant_feature),' variables are found to be almost constant')    \n",
    "    return quasi_constant_feature\n",
    "\n",
    "\n",
    "def scaler(df,scaler=None,train=True, target=None, cols_ignore=None, type=\"Standard\"):\n",
    "\n",
    "  if cols_ignore:\n",
    "    hold = df[cols_ignore].copy()\n",
    "    df = df.drop(cols_ignore,axis=1)\n",
    "  if target:\n",
    "    x = df.drop([target],axis=1).values #returns a numpy array\n",
    "  else:\n",
    "    x = df.values\n",
    "  if train:\n",
    "    if type==\"Standard\":\n",
    "      scal = StandardScaler()\n",
    "    elif type==\"MinMax\":\n",
    "      scal = MinMaxScaler()\n",
    "    scal.fit(x)\n",
    "    x_scaled = scal.transform(x)\n",
    "  else:\n",
    "    x_scaled = scaler.transform(x)\n",
    "  \n",
    "  if target:\n",
    "    df_out = pd.DataFrame(x_scaled, index=df.index, columns=df.drop([target],axis=1).columns)\n",
    "    df_out[target]= df[target]\n",
    "  else:\n",
    "    df_out = pd.DataFrame(x_scaled, index=df.index, columns=df.columns)\n",
    "  \n",
    "  df_out = pd.concat((hold,df_out),axis=1)\n",
    "  if train:\n",
    "    return df_out, scal\n",
    "  else:\n",
    "    return df_out\n",
    "\n",
    "def impute_null_with_tail(df,cols=[]):\n",
    "    \"\"\"\n",
    "    replacing the NA by values that are at the far end of the distribution of that variable\n",
    "    calculated by mean + 3*std\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    for i in cols:\n",
    "        if df[i].isnull().sum()>0:\n",
    "            df[i] = df[i].fillna(df[i].mean()+3*df[i].std())\n",
    "        else:\n",
    "            warn(\"Column %s has no missing\" % i)\n",
    "    return df    \n",
    "  \n",
    "\n",
    "def outlier_detect(data,col,threshold=3,method=\"IQR\"):\n",
    "  \n",
    "    if method == \"IQR\":\n",
    "      IQR = data[col].quantile(0.75) - data[col].quantile(0.25)\n",
    "      Lower_fence = data[col].quantile(0.25) - (IQR * threshold)\n",
    "      Upper_fence = data[col].quantile(0.75) + (IQR * threshold)\n",
    "    if method == \"STD\":\n",
    "      Upper_fence = data[col].mean() + threshold * data[col].std()\n",
    "      Lower_fence = data[col].mean() - threshold * data[col].std()   \n",
    "    if method == \"OWN\":\n",
    "      Upper_fence = data[col].mean() + threshold * data[col].std()\n",
    "      Lower_fence = data[col].mean() - threshold * data[col].std() \n",
    "    if method ==\"MAD\":\n",
    "      median = data[col].median()\n",
    "      median_absolute_deviation = np.median([np.abs(y - median) for y in data[col]])\n",
    "      modified_z_scores = pd.Series([0.6745 * (y - median) / median_absolute_deviation for y in data[col]])\n",
    "      outlier_index = np.abs(modified_z_scores) > threshold\n",
    "      print('Num of outlier detected:',outlier_index.value_counts()[1])\n",
    "      print('Proportion of outlier detected',outlier_index.value_counts()[1]/len(outlier_index))\n",
    "      return outlier_index, (median_absolute_deviation, median_absolute_deviation)\n",
    "\n",
    "\n",
    "    para = (Upper_fence, Lower_fence)\n",
    "    tmp = pd.concat([data[col]>Upper_fence,data[col]<Lower_fence],axis=1)\n",
    "    outlier_index = tmp.any(axis=1)\n",
    "    print('Num of outlier detected:',outlier_index.value_counts()[1])\n",
    "    print('Proportion of outlier detected',outlier_index.value_counts()[1]/len(outlier_index))\n",
    "    \n",
    "    return outlier_index, para\n",
    "    \n",
    "\n",
    "def windsorization(data,col,para,strategy='both'):\n",
    "    \"\"\"\n",
    "    top-coding & bottom coding (capping the maximum of a distribution at an arbitrarily set value,vice versa)\n",
    "    \"\"\"\n",
    "\n",
    "    data_copy = data.copy(deep=True)  \n",
    "    if strategy == 'both':\n",
    "        data_copy.loc[data_copy[col]>para[0],col] = para[0]\n",
    "        data_copy.loc[data_copy[col]<para[1],col] = para[1]\n",
    "    elif strategy == 'top':\n",
    "        data_copy.loc[data_copy[col]>para[0],col] = para[0]\n",
    "    elif strategy == 'bottom':\n",
    "        data_copy.loc[data_copy[col]<para[1],col] = para[1]  \n",
    "    return data_copy\n",
    "\n",
    "\n",
    "def impute_outlier(data,col,outlier_index,strategy='mean'):\n",
    "    \"\"\"\n",
    "    impute outlier with mean/median/most frequent values of that variable.\n",
    "    \"\"\"\n",
    "\n",
    "    data_copy = data.copy(deep=True)\n",
    "    if strategy=='mean':\n",
    "        data_copy.loc[outlier_index,col] = data_copy[col].mean()\n",
    "    elif strategy=='median':\n",
    "        data_copy.loc[outlier_index,col] = data_copy[col].median()\n",
    "    elif strategy=='mode':\n",
    "        data_copy.loc[outlier_index,col] = data_copy[col].mode()[0]   \n",
    "        \n",
    "    return data_copy\n",
    "  \n",
    "\n",
    "### FEATURE ENGINEERING\n",
    "#=============================================================================================\n",
    "\n",
    "def auto_dummy(df, unique=15):\n",
    "  # Creating dummies for small object uniques\n",
    "  if len(df)<unique:\n",
    "    raise ValueError('unique is set higher than data lenght')\n",
    "  list_dummies =[]\n",
    "  for col in df.columns:\n",
    "      if (len(df[col].unique()) < unique):\n",
    "          list_dummies.append(col)\n",
    "          print(col)\n",
    "  df_edit = pd.get_dummies(df, columns = list_dummies) # Saves original dataframe\n",
    "  #df_edit = pd.concat([df[[\"year\",\"qtr\"]],df_edit],axis=1)\n",
    "  return df_edit\n",
    "\n",
    "\n",
    "def binarise_empty(df, frac=80):\n",
    "  # Binarise slightly empty columns\n",
    "  this =[]\n",
    "  for col in df.columns:\n",
    "      if df[col].dtype != \"object\":\n",
    "          is_null = df[col].isnull().astype(int).sum()\n",
    "          if (is_null/df.shape[0]) >frac: # if more than 70% is null binarise\n",
    "              print(col)\n",
    "              this.append(col)\n",
    "              df[col] = df[col].astype(float)\n",
    "              df[col] = df[col].apply(lambda x: 0 if (np.isnan(x)) else 1)\n",
    "  df = pd.get_dummies(df, columns = this) \n",
    "  return df\n",
    "\n",
    "\n",
    "def polynomials(df, feature_list):\n",
    "  for feat in feature_list:\n",
    "    for feat_two in feature_list:\n",
    "      if feat==feat_two:\n",
    "        continue\n",
    "      else:\n",
    "       df[feat+\"/\"+feat_two] = df[feat]/(df[feat_two]-df[feat_two].min()) #zero division guard\n",
    "       df[feat+\"X\"+feat_two] = df[feat]*(df[feat_two])\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def transformations(df,features):\n",
    "  df_new = df[features]\n",
    "  df_new = df_new - df_new.min()\n",
    "\n",
    "  sqr_name = [str(fa)+\"_POWER_2\" for fa in df_new.columns]\n",
    "  log_p_name = [str(fa)+\"_LOG_p_one_abs\" for fa in df_new.columns]\n",
    "  rec_p_name = [str(fa)+\"_RECIP_p_one\" for fa in df_new.columns]\n",
    "  sqrt_name = [str(fa)+\"_SQRT_p_one\" for fa in df_new.columns]\n",
    "\n",
    "  df_sqr = pd.DataFrame(np.power(df_new.values, 2),columns=sqr_name, index=df.index)\n",
    "  df_log = pd.DataFrame(np.log(df_new.add(1).abs().values),columns=log_p_name, index=df.index)\n",
    "  df_rec = pd.DataFrame(np.reciprocal(df_new.add(1).values),columns=rec_p_name, index=df.index)\n",
    "  df_sqrt = pd.DataFrame(np.sqrt(df_new.abs().add(1).values),columns=sqrt_name, index=df.index)\n",
    "\n",
    "  dfs = [df, df_sqr, df_log, df_rec, df_sqrt]\n",
    "\n",
    "  df=  pd.concat(dfs, axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def pca_feature(df, memory_issues=False,mem_iss_component=False,variance_or_components=0.80,drop_cols=None):\n",
    "\n",
    "  if memory_issues:\n",
    "    if not mem_iss_component:\n",
    "      raise ValueError(\"If you have memory issues, you have to preselect mem_iss_component\")\n",
    "    pca = IncrementalPCA(mem_iss_component)\n",
    "  else:\n",
    "    if variance_or_components>1:\n",
    "      pca = PCA(n_components=variance_or_components) \n",
    "    else: # automted selection based on variance\n",
    "      pca = PCA(n_components=variance_or_components,svd_solver=\"full\") \n",
    "  X_pca = pca.fit_transform(df.drop(drop_cols,axis=1))\n",
    "  df = pd.concat((df[drop_cols],pd.DataFrame(X_pca, columns=[\"PCA_\"+str(i+1) for i in range(X_pca.shape[1])])),axis=1)\n",
    "  return df\n",
    "\n",
    "\n",
    "def multiple_lags(df, start=1, end=3,columns=None):\n",
    "  if not columns:\n",
    "    columns = df.columns.to_list()\n",
    "  lags = range(start, end+1)  # Just two lags for demonstration.\n",
    "\n",
    "  df = df.assign(**{\n",
    "      '{}_t_{}'.format(col, t): df[col].shift(t)\n",
    "      for t in lags\n",
    "      for col in columns\n",
    "  })\n",
    "  return df\n",
    "\n",
    "\n",
    "def multiple_rolling(df, windows = [1,2], functions=[\"mean\",\"std\"], columns=None):\n",
    "  windows = [1+a for a in windows]\n",
    "  if not columns:\n",
    "    columns = df.columns.to_list()\n",
    "  rolling_dfs = (df[columns].rolling(i)                                    # 1. Create window\n",
    "                  .agg(functions)                                # 1. Aggregate\n",
    "                  .rename({col: '{0}_{1:d}'.format(col, i)\n",
    "                                for col in columns}, axis=1)  # 2. Rename columns\n",
    "                for i in windows)                                # For each window\n",
    "  df_out = pd.concat((df, *rolling_dfs), axis=1)\n",
    "  da = df_out.iloc[:,len(df.columns):]\n",
    "  da = [col[0] + \"_\" + col[1] for col in  da.columns.to_list()]\n",
    "  df_out.columns = df.columns.to_list() + da \n",
    "\n",
    "  return  df_out                      # 3. Concatenate dataframes\n",
    "\n",
    "\n",
    "def date_features(df, date=\"date\"):\n",
    "  df[date] = pd.to_datetime(df[date])\n",
    "  df[date+\"_month\"] = df[date].dt.month.astype(int)\n",
    "  df[date+\"_year\"]  = df[date].dt.year.astype(int)\n",
    "  df[date+\"_week\"]  = df[date].dt.week.astype(int)\n",
    "  df[date+\"_day\"]   = df[date].dt.day.astype(int)\n",
    "  df[date+\"_dayofweek\"]= df[date].dt.dayofweek.astype(int)\n",
    "  df[date+\"_dayofyear\"]= df[date].dt.dayofyear.astype(int)\n",
    "  df[date+\"_hour\"] = df[date].dt.hour.astype(int)\n",
    "  df[date+\"_int\"] = pd.to_datetime(df[date]).astype(int)\n",
    "  return df\n",
    "\n",
    "def haversine_distance(row, lon=\"latitude\", lat=\"longitude\"):\n",
    "    c_lat,c_long = radians(52.5200), radians(13.4050)\n",
    "    R = 6373.0\n",
    "    long = radians(row['longitude'])\n",
    "    lat = radians(row['latitude'])\n",
    "    \n",
    "    dlon = long - c_long\n",
    "    dlat = lat - c_lat\n",
    "    a = sin(dlat / 2)**2 + cos(lat) * cos(c_lat) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "### MODEL FALIDATION\n",
    "#=============================================================================================\n",
    "\n",
    "def classification_scores(y_test, y_predict, y_prob):\n",
    "\n",
    "  confusion_mat = confusion_matrix(y_test,y_predict)\n",
    "\n",
    "  TN = confusion_mat[0][0]\n",
    "  FP = confusion_mat[0][1]\n",
    "  TP = confusion_mat[1][1]\n",
    "  FN = confusion_mat[1][0]\n",
    "\n",
    "  TPR = TP/(TP+FN)\n",
    "  # Specificity or true negative rate\n",
    "  TNR = TN/(TN+FP) \n",
    "  # Precision or positive predictive value\n",
    "  PPV = TP/(TP+FP)\n",
    "  # Negative predictive value\n",
    "  NPV = TN/(TN+FN)\n",
    "  # Fall out or false positive rate\n",
    "  FPR = FP/(FP+TN)\n",
    "  # False negative rate\n",
    "  FNR = FN/(TP+FN)\n",
    "  # False discovery rate\n",
    "  FDR = FP/(TP+FP)\n",
    "\n",
    "  ll = log_loss(y_test, y_prob) # Its low but means nothing to me. \n",
    "  br = brier_score_loss(y_test, y_prob) # Its low but means nothing to me. \n",
    "  acc = accuracy_score(y_test, y_predict)\n",
    "  print(acc)\n",
    "  auc = roc_auc_score(y_test, y_prob)\n",
    "  print(auc)\n",
    "  prc = average_precision_score(y_test, y_prob) \n",
    "\n",
    "  data = np.array([np.arange(1)]*1).T\n",
    "\n",
    "  df_exec = pd.DataFrame(data)\n",
    "\n",
    "  df_exec[\"Average Log Likelihood\"] = ll\n",
    "  df_exec[\"Brier Score Loss\"] = br\n",
    "  df_exec[\"Accuracy Score\"] = acc\n",
    "  df_exec[\"ROC AUC Sore\"] = auc\n",
    "  df_exec[\"Average Precision Score\"] = prc\n",
    "  df_exec[\"Precision - Bankrupt Firms\"] = PPV\n",
    "  df_exec[\"False Positive Rate (p-value)\"] = FPR\n",
    "  df_exec[\"Precision - Healthy Firms\"] = NPV\n",
    "  df_exec[\"False Negative Rate (recall error)\"] = FNR\n",
    "  df_exec[\"False Discovery Rate \"] = FDR\n",
    "  df_exec[\"All Observations\"] = TN + TP + FN + FP\n",
    "  df_exec[\"Bankruptcy Sample\"] = TP + FN\n",
    "  df_exec[\"Healthy Sample\"] = TN + FP\n",
    "  df_exec[\"Recalled Bankruptcy\"] = TP + FP\n",
    "  df_exec[\"Correct (True Positives)\"] = TP\n",
    "  df_exec[\"Incorrect (False Positives)\"] = FP\n",
    "  df_exec[\"Recalled Healthy\"] = TN + FN\n",
    "  df_exec[\"Correct (True Negatives)\"] = TN\n",
    "  df_exec[\"Incorrect (False Negatives)\"] = FN\n",
    "\n",
    "  df_exec = df_exec.T[1:]\n",
    "  df_exec.columns = [\"Metrics\"]\n",
    "  return df_exec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
